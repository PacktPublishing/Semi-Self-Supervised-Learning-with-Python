{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73f0555-8f46-44f7-90c0-a5c3a5906192",
   "metadata": {},
   "source": [
    "# Understanding Semi-Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7e63f4-f946-4bf9-b616-e2261b527a20",
   "metadata": {},
   "source": [
    "## Technical requirements\n",
    "\n",
    "We will use the following as technical requirements to run the code in this chapter:\n",
    "- Python 3\n",
    "- pip\n",
    "- Tensorflow (with CUDA if you want to train models on GPUs)\n",
    "    - Keras is installed as a dependency to this\n",
    "- scikit-learn Python library\n",
    "    - Numpy is installed as a dependency to this\n",
    "- Pandas Python library\n",
    "- Jupyter notebook if running the code directly from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a6458c-0360-43fb-a6ae-20b6e6eef012",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b8a4ef-bdfe-4ce9-bf26-9a42e51986c3",
   "metadata": {},
   "source": [
    "### For M1+ Macbook (64-bit ARM Based processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c0ba36-1f06-4870-970a-62a895af4de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "! arch -arm64 pip3 install --upgrade pip\n",
    "! arch -arm64 pip3 install tensorflow\n",
    "! arch -arm64 pip3 install -U scikit-learn\n",
    "! arch -arm64 pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891e624e-368e-4a70-b636-a355026dc024",
   "metadata": {},
   "source": [
    "### For Other Computer Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44cb676-5168-49df-9049-3cebbd541081",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip\n",
    "! pip3 install tensorflow\n",
    "! pip3 install -U scikit-learn\n",
    "! pip3 install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759c980-b56f-4f8f-989b-0d3e4af9ecd5",
   "metadata": {},
   "source": [
    "## 2. Pseudo-labeling: Label the unlabeledÂ¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a9b346-9aba-47fa-8ccf-5e0bff750383",
   "metadata": {},
   "source": [
    "### 2.3 Hands-on generating pseudo-labels\n",
    "\n",
    "To illustrate the application of pseudo-labeling, we will create a learning model using the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 different classes, split into 50,000 training images and 10,000 test images. For our purposes to demonstrate the process of pseudo-labeling the unlabeled data, we will treat a small subset of the training images as labeled and the rest as unlabeled. We will go through the process as discussed earlier step by step. We begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d063b0b5-aeba-44fb-90cd-f7038ca1e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models, layers, datasets, utils\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01a9eee-a5ea-499f-a583-37792309da12",
   "metadata": {},
   "source": [
    "#### Train a Baseline Model on Small Labeled Dataset\n",
    "First, we load and preprocess the CIFAR-10 dataset, just as we did in chapter 1, to have the training and testing images and their labels, with each image normalized in range [-1, 1]. Additionally, we also perform one-hot encoding of the class labels to convert them from integer class value to a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca418ca8-a5e2-4d30-aac4-d69347e6ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images / 127.5 - 1\n",
    "test_images = test_images / 127.5 - 1\n",
    "\n",
    "train_labels = utils.to_categorical(train_labels, 10)\n",
    "test_labels = utils.to_categorical(test_labels, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22046805-ae2b-4b6c-933b-59edfd6cf090",
   "metadata": {},
   "source": [
    "Next we use a small subset (1000 images) of the labeled data to be considered as labeled and drop the labels of the remaining labeled data. To ensure all classes are represented in this small subset, we pick the data points equally from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3300f6-5262-40e2-97d4-c627ce631b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labeled_per_class = 100\n",
    "num_classes = 10\n",
    "\n",
    "labeled_indices = []\n",
    "for i in range(num_classes):\n",
    "    indices = np.where(np.argmax(train_labels, axis=1) == i)[0]\n",
    "    labeled_indices.extend(np.random.choice(indices, num_labeled_per_class, replace=False))\n",
    "\n",
    "train_images_labeled_subset = train_images[labeled_indices]\n",
    "train_labels_labeled_subset = train_labels[labeled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d59080-b28d-4aa1-92f2-5182c3f50352",
   "metadata": {},
   "source": [
    "Then, we train a basic Convolutional Neural Network (CNN) model on this small labeled subset. Our CNN would consist of a 3 convolutional and pooling layers, using ReLU as the activation. These are followed by a dropout layer in the final layer after pooling for regularization, and a dense layer for the class label output. Since this is a multi-class logistic regression model, we use softmax as the activation function to predict class probabilities for each input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c4f1d1-904b-4635-b3c8-95a63695dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dropout(0.25),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cd293-0ebf-4b9e-a06a-f60f851195a6",
   "metadata": {},
   "source": [
    "We then specify the optimizer and loss function that would help backpropagate the learning to update model parameters and train the model. We use categorical cross entropy for logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f05c1-4b8d-4a09-a98a-7fae7e756887",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_images_labeled_subset, \n",
    "          train_labels_labeled_subset, \n",
    "          batch_size=32, \n",
    "          epochs=20, \n",
    "          validation_split=0.1\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c07660-cf8a-41a5-b63b-e2d6b29bc8a0",
   "metadata": {},
   "source": [
    "To understand the model performance trained on the small subset of labeled data, we get the accuracy on the test data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1414af5-dd2a-4103-a04a-1de4e6f38a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209ed699-66d0-4ef9-92ba-e53b2e9be851",
   "metadata": {},
   "source": [
    "#### Generate Pseudo-labels\n",
    "Next we use the trained model to predict the labels of the unlabeled data. Here we consider the data from training set as unlabeled data which was not considered labeled previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1447c3b8-3fea-4957-9069-85450fea92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_indices = np.array([i for i in range(train_images.shape[0]) \n",
    "                              if i not in labeled_indices])\n",
    "train_images_unlabeled_subset = train_images[unlabeled_indices]\n",
    "\n",
    "predictions = model.predict(train_images_unlabeled_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840712e9-b273-4385-bc36-bce2f9f40a29",
   "metadata": {},
   "source": [
    "#### Confidence Thresholding\n",
    "We will select predictions with high confidence and use them as labels. In `pseudo_labels`, we store the data index along with the predicted label for only those data points that meet or exceed the confidence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9e1f79-a274-4ccb-b42c-29196b695bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence_threshold = 0.9\n",
    "pseudo_labels = []\n",
    "\n",
    "for i, prediction in enumerate(predictions):\n",
    "    if max(prediction) > confidence_threshold:\n",
    "        pseudo_labels.append((unlabeled_indices[i], np.argmax(prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e7aa4-8ad7-496c-add5-4995dbd00cb7",
   "metadata": {},
   "source": [
    "#### Data Augmentation\n",
    "Once we have filtered out the data with predictions having low confidence score, we can now add the pseudo-labels to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc3857-a82f-493b-a840-069b7cc6a859",
   "metadata": {},
   "outputs": [],
   "source": [
    "pseudo_indices = [index for index, _ in pseudo_labels]\n",
    "train_images_pseudo_labels = utils.to_categorical([label for _, label in pseudo_labels], 10)\n",
    "\n",
    "train_images_combined = np.concatenate([train_images[labeled_indices], train_images[pseudo_indices]])\n",
    "train_labels_combined = np.concatenate([train_labels_labeled_subset, train_images_pseudo_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf680e96-c544-4cee-aae3-c3e1ada45594",
   "metadata": {},
   "source": [
    "#### Model Retraining\n",
    "Finally, we retrain the model on the combined original labeled data and pseudo-labeled data to improve its accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c0fadb-ad45-4316-a1a6-6cc1630f172f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_images_combined, \n",
    "          train_labels_combined, \n",
    "          batch_size=32, \n",
    "          epochs=20, \n",
    "          validation_split=0.1)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8e39cb-3fbe-4a30-91d9-39da955701c9",
   "metadata": {},
   "source": [
    "## 3. Self-training: Using pseudo-labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c9668b-851a-4d34-980d-6fc9669cd0cd",
   "metadata": {},
   "source": [
    "### 3.2 Building a Self-Training Model\n",
    "\n",
    "As discussed earlier, the initial steps of self-training algorithm remain the same as in pseudo-labeling. What is different is the re-training of the model iteratively from scratch. So let us use the same dataset to create a self-training model that is trained iteratively on labeled and unlabeled data. Below we define a method that performs a single iteration of model training on labeled and pseudo-labeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7069680f-7673-43b5-8b6e-a8e470394ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_train(model, train_labeled_images, train_labeled_labels, train_unlabeled_images, threshold):\n",
    "    print(f\"Labeled data size: {train_labeled_images.shape[0]}\")\n",
    "    \n",
    "    model.fit(train_labeled_images, \n",
    "              train_labeled_labels, \n",
    "              batch_size=32, \n",
    "              epochs=20, \n",
    "              validation_split=0.1,\n",
    "              verbose=0)\n",
    "\n",
    "    predictions = model.predict(train_unlabeled_images)\n",
    "    high_confidence_indices = np.max(predictions, axis=1) > threshold\n",
    "    pseudo_labels = np.argmax(predictions[high_confidence_indices], axis=1)\n",
    "    \n",
    "    labeled_images_combined = np.concatenate((train_labeled_images, \n",
    "                                              train_unlabeled_images[high_confidence_indices]))\n",
    "    labeled_labels_combined = np.concatenate((train_labeled_labels, \n",
    "                                              utils.to_categorical(pseudo_labels, 10)))\n",
    "\n",
    "    remaining_unlabeled_indices = [i for i in range(train_unlabeled_images.shape[0]) \n",
    "                                           if i not in high_confidence_indices]\n",
    "    remaining_unlabeled_images = np.array(train_unlabeled_images[remaining_unlabeled_indices])\n",
    "\n",
    "    return labeled_images_combined, labeled_labels_combined, remaining_unlabeled_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89910cab-464f-4987-95ae-7083ccf41dcf",
   "metadata": {},
   "source": [
    "In the above code, we pass the CNN model, the originally labeled data as train_labeled_images, original labels as train_labeled_labels and the unlabeled data as train_unlabeled_images. The confidence threshold is also passed to assign pseudo-labels to unlabeled data and include them in the labeled dataset. We then return the combined labeled dataset including the pseudo labeled data, their labels and the set of the remaining unlabeled data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bd12ae-75e0-4435-9eb8-0401b79faf93",
   "metadata": {},
   "source": [
    "After the initial model is trained, we get the pseudo-labels based on the threshold, add it to the expanded labeled set and return the 3 sets of labeled data, labels and indices of unlabeled data. Next, let's iteratively re-train the model, by incrementally reducing the confidence threshold in each iteration. We first define a function get_model() that returns a model with resetted weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e351ca41-ca12-43f0-ad10-9921ccd4c19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.25),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40854a2-8f0b-45e3-be13-f32d02dd8693",
   "metadata": {},
   "source": [
    "Then, we self-train the model iteratively by reducing the threshold by 5% in each iteration. We start by initializing the labeled and unlabeled dataset and setting the threshold to 95% and number of iterations to 3. Over each iteration, we closely monitor the model's accuracy and convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13658c4-4260-483c-a09c-3809fddfa67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.95\n",
    "iterations = 3\n",
    "\n",
    "labeled_images_combined = train_images_labeled_subset\n",
    "labeled_labels_combined = train_labels_labeled_subset\n",
    "remaining_unlabeled_images = train_images_unlabeled_subset\n",
    "\n",
    "for iteration in range(iterations):\n",
    "    model = get_model()\n",
    "    \n",
    "    labeled_images_combined, labeled_labels_combined, remaining_unlabeled_images = self_train(model, \n",
    "                                           labeled_images_combined, \n",
    "                                           labeled_labels_combined, \n",
    "                                           remaining_unlabeled_images, \n",
    "                                           threshold)\n",
    "\n",
    "    threshold *= 0.95\n",
    "\n",
    "    loss, accuracy = model.evaluate(test_images, test_labels)\n",
    "    print(f\"Iteration {iteration+1}, Loss: {loss}, Accuracy: {accuracy}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dac6b6-071e-405f-9af9-e5535706cd1b",
   "metadata": {},
   "source": [
    "As can be seen that the performance of the model over each iteration gets better even though the initial set of labeled data is still limited."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d5b611-094e-4c57-a36b-0ef22de52532",
   "metadata": {},
   "source": [
    "## 4. Co-Training: A multi-model approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f476b76-c3f5-4e3f-9622-02efcddad98d",
   "metadata": {},
   "source": [
    "### 4.2 Implementing a Co-Training Model\n",
    "\n",
    "In this section, we would perform a hand-on exercise to train a co-training model. We will use UCI Multiple Features dataset [https://archive.ics.uci.edu/dataset/72/multiple+features] for this exercise. The UCI Multiple Features dataset is a collection of features of handwritten numerals 0-9. We can leverage six different feature sets (views) that are inherently provided by the dataset, extracted from a collection of Dutch utility maps. For our demonstration, let's use two of these views, specifically \"Fourier coefficients of the character shapes\" and \"Karhunen-Love coefficients\" which are available in the UCI repository as `mfeat-fourier` and `mfeat-karhunen`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a56cd49-011f-4437-be28-12f0e91620e9",
   "metadata": {},
   "source": [
    "We will start by first importing the necessary libraries for the model training. The `fetch_openml` is used to load the UCI Multiple Features dataset with different views, `train_test_split` is used to split the dataset in training and testing sets, `StandardScaler` is used to standardize the dataset to zero mean and unit variance. We also use Gaussian Naive Bayes (`GaussianNB`) classifiers and `accuracy_score` to train and evaluate the co-training models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6573e5c-6956-412f-8cbc-3521e7283217",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886b6b78-d1c4-451a-bdee-b6c70b4035fa",
   "metadata": {},
   "source": [
    "Let us next load the 76 Fourier coefficients of the character shapes `mfeat-fourier` and 64 Karhunen-Love coefficients `mfeat-karhunen` as two views of the dataset. Assuming both datasets share the same order of instances, we only load the target labels from one view since both the views will have same target labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c527af2-c9e8-45e1-a7c6-edb9d6ec740a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier = fetch_openml('mfeat-fourier', version=1)\n",
    "karhunen = fetch_openml('mfeat-karhunen', version=1)\n",
    "\n",
    "X_fourier, y = fourier.data, fourier.target\n",
    "X_karhunen = karhunen.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0419f9-56dd-4258-bf98-a4efaba67a5c",
   "metadata": {},
   "source": [
    "We then standardize the dataset to zero mean and unit variance using `sklearn`'s `StandardScaler`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde60766-9cb5-4c6d-a79e-ba2315d80f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_fourier = StandardScaler()\n",
    "scaler_karhunen = StandardScaler()\n",
    "X_fourier_scaled = scaler_fourier.fit_transform(X_fourier)\n",
    "X_karhunen_scaled = scaler_karhunen.fit_transform(X_karhunen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95346df0-1d9a-4b23-b002-9bd1c1d91faf",
   "metadata": {},
   "source": [
    "Now we split the dataset into training and testing data as follows. We retain 30% of the data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65147c2c-a579-4bf7-b43e-caac0474d985",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_fourier, X_test_fourier, y_train, y_test = train_test_split(X_fourier_scaled, y, test_size=0.3, random_state=42)\n",
    "X_train_karhunen, X_test_karhunen = train_test_split(X_karhunen_scaled, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"Shape of Fourier based train data {np.shape(X_train_fourier)}\")\n",
    "print(f\"Shape of Fourier based test data {np.shape(X_test_fourier)}\")\n",
    "print(f\"Shape of Karhunen based train data {np.shape(X_train_karhunen)}\")\n",
    "print(f\"Shape of Karhunen based test data {np.shape(X_test_karhunen)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa191ea-edf4-479c-8ec3-adbfdb340075",
   "metadata": {},
   "source": [
    "As can be noticed, the first view of \"Fourier coefficients of the character shapes\" has 76 features while the second view of \"Karhunen-Love coefficients\" has 64 features. The 2000 data points are divided into 1400 training data and 600 test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a25797-63f4-4c95-9110-185e401eb20b",
   "metadata": {},
   "source": [
    "Let's split the above data into 4 different sets. For each view, we will obtain a small labeled subset and use larger subset as unlabeled to simulate the co-training method. For our exercise, we consider first 300 data points from training data as labeled and rest 1100 data points as unlabeled. You can choose a different number for obtaining labeled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dade20e-e54f-47b2-821b-14fb9773f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELED = 300\n",
    "\n",
    "X_labeled_fourier = X_train_fourier[:NUM_LABELED]\n",
    "X_unlabeled_fourier = X_train_fourier[NUM_LABELED:]\n",
    "X_labeled_karhunen = X_train_karhunen[:NUM_LABELED]\n",
    "X_unlabeled_karhunen = X_train_karhunen[NUM_LABELED:]\n",
    "y_labeled = y_train[:NUM_LABELED]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3980296c-c959-46f2-a1cb-76c797d667ef",
   "metadata": {},
   "source": [
    "Now, we'll create our co-training function to handle two different feature sets, ensuring each classifier trains on distinct information and train the models on two different views of the dataset we have created earlier. In the `co_train` function, we take labeled and unlabeled data from two views along with the initial target labels. We train the models on the initial target labels and then in each iteration, we get the pseudo-labels from the two classifiers, select the high confidence pseudo-labels and then append the high confidence pseudo-labels from one classifier to the labeled dataset of the other classifier. We train the models again in the new subset of training data that has pseudo-labeled data with it. For the purpose of this demonstration, we train both the classifier models as Gaussian Naive Bayes. We also define `evaluate_models` function that evaluates the two classifiers on the test data. When the label from both the classifiers agree, we select the label from `classifier1`. But if there is a disagreement on the label, we would randomly select the label from either of the classifiers with 0.5 probability of label selection from each of the 2 classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a094d1-63ca-4f1a-91ca-fe579b32f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(classifier1, classifier2, X_test_fourier, X_test_karhunen):\n",
    "    predictions1 = classifier1.predict(X_test_fourier)\n",
    "    predictions2 = classifier2.predict(X_test_karhunen)\n",
    "\n",
    "    random_mask = np.random.rand(len(predictions1)) > 0.5 \n",
    "    final_pred = np.where(predictions1 == predictions2, predictions1, np.where(random_mask, predictions1, predictions2))\n",
    "\n",
    "    print(\"Accuracy of Co-Training model:\", accuracy_score(y_test, final_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8f8b58-8df4-4229-8d78-e63d6dd62382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_train(X_labeled_view1, X_labeled_view2, y_train, X_unlabeled_view1, X_unlabeled_view2, X_test_fourier, X_test_karhunen, n_iterations=10, threshold=0.9):\n",
    "    classifier1 = GaussianNB()\n",
    "    classifier2 = GaussianNB()\n",
    "    \n",
    "    classifier1.fit(X_labeled_view1, y_train)\n",
    "    classifier2.fit(X_labeled_view2, y_train)\n",
    "\n",
    "    print(\"For base classifiers\")\n",
    "    evaluate_models(classifier1, classifier2, X_test_fourier, X_test_karhunen)\n",
    "\n",
    "    for iteration in range(n_iterations):\n",
    "        pseudo_labels1 = classifier1.predict(X_unlabeled_view1)\n",
    "        pseudo_labels2 = classifier2.predict(X_unlabeled_view2)\n",
    "        \n",
    "        confidences1 = classifier1.predict_proba(X_unlabeled_view1).max(axis=1) > threshold\n",
    "        confidences2 = classifier2.predict_proba(X_unlabeled_view2).max(axis=1) > threshold\n",
    "\n",
    "        new_X1 = X_unlabeled_view1[confidences2]\n",
    "        new_y1 = pseudo_labels2[confidences2]\n",
    "        new_X2 = X_unlabeled_view2[confidences1]\n",
    "        new_y2 = pseudo_labels1[confidences1]\n",
    "\n",
    "        pseudo_labeled_X1 = np.concatenate((X_labeled_view1, new_X1))\n",
    "        y_train1 = np.concatenate((y_train, new_y1))\n",
    "        pseudo_labeled_X2 = np.concatenate((X_labeled_view2, new_X2))\n",
    "        y_train2 = np.concatenate((y_train, new_y2))\n",
    "\n",
    "        classifier1.fit(pseudo_labeled_X1, y_train1)\n",
    "        classifier2.fit(pseudo_labeled_X2, y_train2)\n",
    "\n",
    "        print(\"Iteration: \", iteration+1)\n",
    "        evaluate_models(classifier1, classifier2, X_test_fourier, X_test_karhunen)\n",
    "\n",
    "    return classifier1, classifier2\n",
    "\n",
    "classifier1, classifier2 = co_train(X_labeled_fourier, \n",
    "                                    X_labeled_karhunen, \n",
    "                                    y_labeled, \n",
    "                                    X_unlabeled_fourier, \n",
    "                                    X_unlabeled_karhunen, \n",
    "                                    X_test_fourier, \n",
    "                                    X_test_karhunen,\n",
    "                                    threshold=0.95)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d4928-971d-4ec1-b4de-76a6253ead13",
   "metadata": {},
   "source": [
    "As we can notice, the accuracy of the models was slightly better than the base classifiers. However, the accuracy keeps swinging between iterations. That could be because in each iteration, we resolve the confict in case of label disagreement using a randomizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3b4bbd-c8af-4b24-bab3-063902fda447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_venv",
   "language": "python",
   "name": "book_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
