{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1fb655a-165f-4c0b-a6ef-c18efbdd982b",
   "metadata": {},
   "source": [
    "# Learning from Unlabeled Data: Rise of Semi-Supervised and Self-Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ad4fc6-8791-4ed2-84b1-a0dafd7d4fb5",
   "metadata": {},
   "source": [
    "## Technical requirements\n",
    "\n",
    "We will use the following as technical requirements to run the code in this chapter:\n",
    "- Python 3\n",
    "- pip\n",
    "- Tensorflow (with CUDA if you want to train models on GPUs)\n",
    "    - Keras is installed as a dependency to this\n",
    "- scikit-learn Python library\n",
    "    - Numpy is installed as a dependency to this\n",
    "- Jupyter notebook if running the code directly from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d9baf5c-3c15-4027-845f-b0e48558cc07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./book_venv/lib/python3.10/site-packages (24.0)\n"
     ]
    }
   ],
   "source": [
    "! python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50eca89-3c16-4e9a-84ef-9c03987f421c",
   "metadata": {},
   "source": [
    "### For M1+ Macbook (64-bit ARM Based processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a3d11-840b-4e9d-b4f4-d65251539321",
   "metadata": {},
   "outputs": [],
   "source": [
    "! arch -arm64 pip3 install --upgrade pip\n",
    "! arch -arm64 pip3 install tensorflow\n",
    "! arch -arm64 pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b53183a-e958-45e1-b8b5-7441ce2fe33a",
   "metadata": {},
   "source": [
    "### For Other Computer Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efcb74ba-2928-4754-937d-c82123556992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./book_venv/lib/python3.10/site-packages (24.0)\n",
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./book_venv/lib/python3.10/site-packages (from tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in ./book_venv/lib/python3.10/site-packages (from tensorflow) (67.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (4.10.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (1.62.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (3.1.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (0.36.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in ./book_venv/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./book_venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in ./book_venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in ./book_venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: optree in ./book_venv/lib/python3.10/site-packages (from keras>=3.0.0->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./book_venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./book_venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./book_venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./book_venv/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./book_venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./book_venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./book_venv/lib/python3.10/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./book_venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./book_venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./book_venv/lib/python3.10/site-packages (from rich->keras>=3.0.0->tensorflow) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./book_venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n",
      "Using cached tensorflow-2.16.1-cp310-cp310-macosx_12_0_arm64.whl (227.0 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.16.1\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in ./book_venv/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.13.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.4.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.4.1.post1-cp310-cp310-macosx_12_0_arm64.whl (10.4 MB)\n",
      "Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Downloading scipy-1.13.0-cp310-cp310-macosx_12_0_arm64.whl (30.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.3/30.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 scipy-1.13.0 threadpoolctl-3.4.0\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade pip\n",
    "! pip3 install tensorflow\n",
    "! pip3 install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9d2fd6e-8275-4118-9be7-998d878922b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d96b57-272e-474a-a30b-bf673d970490",
   "metadata": {},
   "source": [
    "## 1. Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb65719-981b-4b73-b3eb-d4a8b82a98de",
   "metadata": {},
   "source": [
    "### 1.1 Building a Machine Learning Model\n",
    "\n",
    "We usually divide the given data into 2 subsets - one for training and other for testing. Below we will use CIFAR-10 dataset to build a classifier model and a clustering model. You can see we first load the train and test images and labels and then normalize each image to be in range [-1, 1] before it could be fed into our ML model for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c4e6347-42cc-4eff-a0cd-5df62477a190",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "train_images = train_images / 127.5 - 1\n",
    "test_images = test_images / 127.5 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96afcd93-67bf-4cf2-b519-3dc44686cdc1",
   "metadata": {},
   "source": [
    "Next, we need to decide on the right learning method and algorithm that would solve the problem at hand. If the problem is to predict classes for each test image, we would train the model using labels of each image through a convolutional neural network as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010eede7-06cc-481f-9aab-13a7ae6e773b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1d4cf3-6771-4bec-85d0-adaec401ebbb",
   "metadata": {},
   "source": [
    "Next, we would select an appropriate loss function and optimization technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0434920b-cdc4-4e75-a01b-180eabf1a34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2309806-77eb-45bb-acb9-a940af16f96d",
   "metadata": {},
   "source": [
    "Finally, we train the model, validate and test it using the right evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4312414e-77cd-4557-9afc-df470246958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.4111 - loss: 1.6087 - val_accuracy: 0.5973 - val_loss: 1.1089\n",
      "Epoch 2/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.6343 - loss: 1.0351 - val_accuracy: 0.6780 - val_loss: 0.9265\n",
      "Epoch 3/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7032 - loss: 0.8501 - val_accuracy: 0.7014 - val_loss: 0.8589\n",
      "Epoch 4/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7402 - loss: 0.7432 - val_accuracy: 0.7215 - val_loss: 0.8004\n",
      "Epoch 5/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 11ms/step - accuracy: 0.7698 - loss: 0.6629 - val_accuracy: 0.7259 - val_loss: 0.8055\n",
      "Epoch 6/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.7948 - loss: 0.5904 - val_accuracy: 0.7385 - val_loss: 0.7672\n",
      "Epoch 7/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.8160 - loss: 0.5343 - val_accuracy: 0.7362 - val_loss: 0.8105\n",
      "Epoch 8/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.8324 - loss: 0.4847 - val_accuracy: 0.7369 - val_loss: 0.8184\n",
      "Epoch 9/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 10ms/step - accuracy: 0.8472 - loss: 0.4359 - val_accuracy: 0.7297 - val_loss: 0.8564\n",
      "Epoch 10/10\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 10ms/step - accuracy: 0.8630 - loss: 0.3923 - val_accuracy: 0.7284 - val_loss: 0.9209\n",
      "313/313 - 1s - 3ms/step - accuracy: 0.7284 - loss: 0.9209\n",
      "Test accuracy: 0.7283999919891357\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=10, validation_data=(test_images, test_labels))\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
    "print(f'Test accuracy: {test_acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5bd415-3f6d-4df1-89e4-4a4e5894dc4b",
   "metadata": {},
   "source": [
    "We'll get the output as follows for each epoch and final test accuracy.\n",
    "\n",
    "But if the problem is to create clusters of images that represent the same group of entities, we would not use any labels for that purpose. So we collate training and test images and flatten them so that we can use a K-means clustering algortihm to get image clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db6e3c7f-f46c-4628-953e-0c95786534b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 3 6 3 5 4 4 9 5 6 2 3 6 5 7 7 6 2 1 8 4 6 5 5 3 5 6 4 2 0 3 5 1 4 4 9\n",
      " 5 3 0 3 8 4 2 0 5 0 0 8 8 1 0 1 2 6 7 1 5 5 3 0 9 0 5 0 9 4 7 5 3 4 5 2 5\n",
      " 4 4 7 0 0 8 4 4 0 3 1 6 1 1 5 4 4 5 9 0 3 2 0 5 5 6]\n"
     ]
    }
   ],
   "source": [
    "images = np.concatenate((train_images, test_images))\n",
    "images_flattened = images.reshape(images.shape[0], -1)\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=42)\n",
    "cluster_assignments = kmeans.fit_predict(images_flattened)\n",
    "\n",
    "print(cluster_assignments[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "book_venv",
   "language": "python",
   "name": "book_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
