{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a4d704-ec5a-4177-95ff-27b0ffc4c26b",
   "metadata": {},
   "source": [
    "# Understanding Semi-Supervised Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7f2756-d32b-4c0e-a7f0-232209c4bf7b",
   "metadata": {},
   "source": [
    "## Technical requirements\n",
    "\n",
    "We will use the following as technical requirements to run the code in this chapter:\n",
    "- Python 3.9 or above\n",
    "- pip\n",
    "- Tensorflow (with CUDA if you want to train models on GPUs)\n",
    "    - Keras is installed as a dependency to this\n",
    "- scikit-learn Python library\n",
    "    - Numpy is installed as a dependency to this\n",
    "- Matplotlib library\n",
    "- Hugging Face's transformers library\n",
    "- Langchain library\n",
    "- Jupyter notebook if running the code directly from Jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abe5ea-9f12-483e-99f8-55e51bd7ac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbaf584-cb91-4558-b974-ec9ecec759b1",
   "metadata": {},
   "source": [
    "### For M1+ Macbook (64-bit ARM Based processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42fe1e-e0fc-472c-91c0-6398b4e791bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "! arch -arm64 pip3 install --upgrade pip\n",
    "! arch -arm64 pip3 install tensorflow\n",
    "! arch -arm64 pip3 install -U scikit-learn\n",
    "! arch -arm64 pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80614a52-56b7-4df9-a8d7-1f9a233974ed",
   "metadata": {},
   "source": [
    "### For Other Computer Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c260713-5d1d-4073-8f05-e7e297fc6519",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade pip\n",
    "! pip3 install tensorflow\n",
    "! pip3 install -U scikit-learn\n",
    "! pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76978ea-4308-40d7-854c-da6dfb129498",
   "metadata": {},
   "source": [
    "## 5. Hands-on Fine Tuning in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7bcbb-f686-40ca-97f4-139410167648",
   "metadata": {},
   "source": [
    "### 5.1 Training pre-trained model\n",
    "First we import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c290616-1880-4f27-84c3-c08f33b7896a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7719dedc-7fac-4ba1-a470-65c12cafdf27",
   "metadata": {},
   "source": [
    "Then we load and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162b10e4-01d2-432c-acae-46ced4559f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "    # One-hot encode the labels\n",
    "    y_train = to_categorical(y_train, num_classes=10)\n",
    "    y_test = to_categorical(y_test, num_classes=10)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "x_train, y_train, x_test, y_test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d250f2-7ee1-448b-af6e-a68472908c54",
   "metadata": {},
   "source": [
    "We then create a function to rotate images and prepare labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6034ab6-7625-42b6-8f8d-9ca7ab09912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rotated_images(images):\n",
    "    angles = [0, 90, 180, 270]\n",
    "    angle_labels = np.random.randint(0, 3, size=len(images))\n",
    "    rotated_images = np.array([\n",
    "        tf.image.rot90(images[i], k=angle_labels[i]).numpy()\n",
    "        for i in range(len(images))\n",
    "    ])\n",
    "    return rotated_images, angle_labels\n",
    "\n",
    "x_train_rotated, y_train_rotated = create_rotated_images(x_train)\n",
    "x_test_rotated, y_test_rotated = create_rotated_images(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8542af-2200-490d-9941-3be7e398a97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_rotated_images(images, labels):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    for i in range(10):\n",
    "        idx = np.random.randint(0, images.shape[0])\n",
    "        plt.subplot(2, 5, i + 1)\n",
    "        plt.imshow(images[idx])\n",
    "        plt.title(f'Rotation: {labels[idx]*90}Â°')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_rotated_images(x_train_rotated, y_train_rotated)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2ff617-595c-4ffc-9bef-bc0c18745139",
   "metadata": {},
   "source": [
    "For the ML model, we'll use a simple CNN model that outputs four classes corresponding to the four rotation angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712157e-2e9e-4cfa-b5d4-b153140a94f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, (3, 3), padding='same', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = layers.MaxPooling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(128, (3, 3), padding='same', activation='relu')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(4, activation='softmax')(x)\n",
    "    \n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "pretext_model = build_model(x_train_rotated[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de70e525-64fe-4c1f-91fc-45e258a3a5c5",
   "metadata": {},
   "source": [
    "Then we train the model as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1034231-aabe-4d5b-9227-4391e5323323",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretext_model.fit(x_train_rotated, y_train_rotated, epochs=10, batch_size=64,\n",
    "                    validation_data=(x_test_rotated, y_test_rotated))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419a0c27-93bb-4414-8bfc-b657b2850960",
   "metadata": {},
   "source": [
    "### 5.2 Fine tuning classification model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50b2ca-831f-4ba7-afbd-d41cb0f3bbdc",
   "metadata": {},
   "source": [
    "Now that we have a pre-trained model, we can discard some of the last few dense layers and retain the first few layers to use them in a model that would predict the label of the dataset. We will freeze the layers from the pre-trained models so that they are not retrained when we fit the transfer model. For comparison, we will also train another model from scratch and see the accuracy of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ac6962-b85c-44bb-adb5-1e010d4754d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_transfer_model(base_model, input_shape, num_classes):\n",
    "    # Remove the last 2 dense layers and flatten layer\n",
    "    base_model = models.Model(inputs=base_model.input,\n",
    "                              outputs=base_model.get_layer(index=-5).output)\n",
    "    \n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False  # Freeze the convolutional layers\n",
    "\n",
    "    new_model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    new_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return new_model\n",
    "\n",
    "transfer_model = build_transfer_model(pretext_model, x_train[0].shape, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f87259-5fb1-4f5e-8fd5-c790f287130e",
   "metadata": {},
   "source": [
    "Next we fine tune the transfer model for classification on the original data with class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3b8813-d092-4913-a209-a06cc64bed9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f53bd4-3fbf-4ec2-84b8-bc7d37da87f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = transfer_model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee13639-7bcc-482b-b91e-d84b0e7d7936",
   "metadata": {},
   "source": [
    "### 5.3 Training classification model from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3222172f-4652-4b50-ba0c-8fa4e19dd0b3",
   "metadata": {},
   "source": [
    "Let's train a classification model from scratch for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de12c4c-7d62-4865-8d19-017a19f9807d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_scratch_model(image, num_classes):\n",
    "    scratch_model = build_model(image.shape)\n",
    "    \n",
    "    scratch_model = models.Model(inputs=scratch_model.input,\n",
    "                                  outputs=scratch_model.get_layer(index=-2).output)\n",
    "    \n",
    "    scratch_model = models.Sequential([\n",
    "            scratch_model,\n",
    "            layers.Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "    \n",
    "    scratch_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return scratch_model\n",
    "\n",
    "scratch_model = build_scratch_model(x_train[0], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fc2a7-f84a-443e-943a-a46186bbfca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch_model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcd6c90-cef5-4400-98f2-76cc4aaf8e23",
   "metadata": {},
   "source": [
    "As you would notice, when we train the model from scratch, it would take a lot more time for model training for each epoch (evident by time per step). We also note that the model accuracy is comparable while we get a massive gains in training time and thereby computational resources. This helps us understand how using pre-trained models can be of great benefit and can be applied to many of the use cases in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96401c-3c2b-4de0-98f2-af7e4a33a23a",
   "metadata": {},
   "source": [
    "### 5.4 Introduction to Hugging Face and LangChain Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5ba2bc-23ba-4cab-beb0-04390471d6ce",
   "metadata": {},
   "source": [
    "#### 5.4.1 Hugging Face _transformers_ Library\n",
    "To get started, you first need to install the transformers library. It's generally used with a backend like PyTorch or TensorFlow. Here, I'll show an example using PyTorch. We'll also install backwards-compatible tf-keras package used by _transformers_ library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec981a70-7a30-4462-9aa5-852e1b464f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For M1+ Macbook (64-bit ARM Based processor)\n",
    "! arch -arm64 pip3 install transformers torch\n",
    "! arch -arm64 pip3 install tf-keras\n",
    "\n",
    "## For Other Computer Systems\n",
    "# ! pip3 install transformers torch\n",
    "# ! pip3 install tf-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea50c1d-e509-4b3c-b64a-ebf6b465fa0c",
   "metadata": {},
   "source": [
    "One of the simplest uses of the _transformers_ library is to load a pre-trained model and use it for inference. Let's use the BERT model for a sentiment analysis task. We will first import the _pipeline_ module from the _transformers_ library. The _pipeline_ module abstract a lot of details away for us and makes it easy to load a pre-trained model for NLP tasks. Then we use the task _sentiment_analysis_ to automatically select and load a model that is trained on sentiment analysis tasks. There are a lot of other tasks available like _text-generation_ (full list: https://huggingface.co/docs/transformers/en/main_classes/pipelines#transformers.pipeline.task) and we can also specify a specific model to be selected by the _pipeline_. We then apply the classifier model returned by the _pipeline_ to a sentence to determine its sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196ed65-788e-4262-af05-eb071d635266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as tf_keras\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis')\n",
    "\n",
    "text = \"I love learning new things about AI and I am excited to read the book on Self-Supervised learning.\"\n",
    "result = sentiment_pipeline(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfae14e-e8f9-4ef5-84a9-7f1b6a1b77df",
   "metadata": {},
   "source": [
    "The output is a list of dictionaries, each containing the label (e.g., 'POSITIVE' or 'NEGATIVE') and score (a confidence level between 0 and 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7b076ca-6892-4e56-8553-5b19e20e406c",
   "metadata": {},
   "source": [
    "Under the hood, the _pipeline_ selects a default model and tokenizer for the specified task and then the sentence provided as an input is first tokenized to convert it to model appropriate input and then fed into the model for predictions or generations. In the following code, we will specify the model and the tokenizer used by the BERT model for English language. We will learn more about the tokenizers and BERT model in Chapter 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b1ee0-52b6-4cba-908a-e9ccf3dc7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"I love learning new things about AI and I am excited to read the book on Self-Supervised learning.\"\n",
    "result = sentiment_pipeline(text)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02b8127-b48e-453c-9d75-2d10e9e3cbc8",
   "metadata": {},
   "source": [
    "As you can notice, the result is exactly the same. So now you know what goes into a pipeline from a transformers library from Hugging Face. The importance of transformers library becomes apparent when they are being used to fine-tune the pre-trained models using neural networks using existing modules from Tensorflow or PyTorch. You can read more on how to fine-tune a pretrained model in Tensorflow with Keras or native PyTorch from the official documentation from Hugging Face: https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb27d1-2d95-4300-97f8-a5c44bf83d92",
   "metadata": {},
   "source": [
    "#### 5.4.2 LangChain Library\n",
    "We need to first install langchain package along with necessary dependencies. Since LangChain can integrate with several backend providers like OpenAI for GPT models, you might also need API keys for those services if you plan to use their models. You can create the OpenAI API account to generate API access key here: https://platform.openai.com/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae10a2ab-22c6-4df2-847b-3301c22735fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For M1+ Macbook (64-bit ARM Based processor)\n",
    "! arch -arm64 pip3 install langchain langchain-community langchain-core openai\n",
    "\n",
    "## For Other Computer Systems\n",
    "# ! pip3 install langchain langchain-community langchain-core openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757b122d-5243-4e80-93d4-f78349d0911a",
   "metadata": {},
   "source": [
    "To demonstrate the basic capabilities of LangChain, let's create a simple application that uses an OpenAI GPT model to answer questions based on a provided text. This example assumes you have an API key for OpenAI. We first import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3840f16-d857-4cfa-8b68-a58a0ef84e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain, SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a55f11-1e6f-4d56-b8d4-b8eeeb2b692c",
   "metadata": {},
   "source": [
    "We set the API key for OpenAI as an environment variable _OPENAI_API_KEY_ which is fetched directly by the _OpenAI_ class instance. The model name is passed to the _OpenAI_ class to initialize the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8a59c0-c327-4e0e-9940-ef0b01ab93e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os\n",
    "OPENAI_API_KEY = getpass()\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a724a9-daf4-493b-895b-1d67eb3fe57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "603ff416-7214-44d0-98e4-fe18e68bfcf8",
   "metadata": {},
   "source": [
    "_PromptTemplate_ is designed to structure the input to the language model in a way that fits the task. For our example, we want to get the book title from its genre. The template includes placeholders for context and the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f553160-dd31-4037-baef-d6df8b148312",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_title = PromptTemplate(\n",
    "    input_variables=['genre'],\n",
    "    template=\"Suggest a book title from the genre {genre}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75657de6-6277-48ca-98cd-242f49c39023",
   "metadata": {},
   "source": [
    "_LLMChain_ class in LangChain is a type of chain that runs queries against a LLM. Chains in LangChain orchestrate the flow, sending formatted prompts to the language model and processing the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef89f591-e4ac-4603-bd17-4b036f0fb4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_title,\n",
    "    output_key=\"book_title\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35013d4a-fe27-4597-99bf-a3706e912933",
   "metadata": {},
   "source": [
    "The _run()_ method takes the question and context, processes them through the configured chain, and returns the generated answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa996816-f539-4da3-8194-2996c61ae44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "genre = \"Romance\"\n",
    "answer = title_chain.run(genre=genre)\n",
    "\n",
    "print(\"Answer:\", answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b14d4-0089-4b07-a606-fd93afcf9e23",
   "metadata": {},
   "source": [
    "We can subsequently chain the output from the LLM as an input to another prompt and get the final result as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d6c0c3-3aad-4073-9f3e-675d6e29e2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_content = PromptTemplate(\n",
    "    input_variables=['book_title'],\n",
    "    template=\"Generate a sentence for the book titled {book_title}.\"\n",
    ")\n",
    "\n",
    "content_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template_content,\n",
    "    output_key=\"book_content\"\n",
    ")\n",
    "\n",
    "chain = SequentialChain(\n",
    "        chains=[title_chain, content_chain],\n",
    "        input_variables=['genre'],\n",
    "        output_variables=['book_title', 'book_content']\n",
    "    )\n",
    "\n",
    "content_from_seq_chain = chain({'genre':'Romance'})\n",
    "print(content_from_seq_chain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a4384b-0b4d-49e9-b37a-8e70534633c1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
